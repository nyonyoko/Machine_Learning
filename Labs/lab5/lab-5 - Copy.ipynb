{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1039540-0903-4959-b16f-58fa660d635f",
   "metadata": {},
   "source": [
    "# Fundamentals of Machine Learning (CSCI-UA.473)\n",
    "\n",
    "## Lab 5: Boosting and Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff9bb9-8ddc-4a6b-ae3d-3324ec811f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and load dataset for this exercise - pip install palmerpenguins\n",
    "from palmerpenguins import load_penguins\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# This function returns a pandas dataframe by default (use return_X_y to get it in two numpy arrays)\n",
    "penguins = load_penguins().dropna()\n",
    "X = penguins[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']]\n",
    "y = penguins['species']\n",
    "print(X.shape, y.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa89b7a-0f7e-40f2-95e9-7f34c6dcb8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data.  DO NOT TOUCH THE TEST DATA FROM HERE ON!!\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y, test_size = 0.2) # 0.2 is 20% test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6358d40-1b3e-4d53-828c-47a526207d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we implement gradient boosting, in particular the Adaboost algorithm.\n",
    "# Remember, gradient boosting algorithms involve iteratively improving the decision trees\n",
    "# and hence involve a learning rate similar to logistic regressions.\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Create and fit an AdaBoosted decision tree\n",
    "bdt = AdaBoostClassifier(\n",
    "    tree.DecisionTreeClassifier(max_depth=1), algorithm=\"SAMME\", n_estimators=2000, learning_rate=1\n",
    ")\n",
    "bdt.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fde4ef-5d37-492f-864e-df548f9af7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bdt.predict(X_test)\n",
    "print(np.sum(preds == y_test)/len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c12fb7c-464a-4318-bdd0-4004f964198b",
   "metadata": {},
   "source": [
    "### Using a synthetic dataset to visualize effects of different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee2241-d8ab-4da0-90dd-b9db5f6efd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "# Construct dataset\n",
    "X1, y1 = make_gaussian_quantiles(\n",
    "    cov=2.0, n_samples=200, n_features=2, n_classes=2, random_state=1\n",
    ")\n",
    "X2, y2 = make_gaussian_quantiles(\n",
    "    mean=(3, 3), cov=1.5, n_samples=300, n_features=2, n_classes=2, random_state=1\n",
    ")\n",
    "X = np.concatenate((X1, X2))\n",
    "y = np.concatenate((y1, -y2+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126eadb-f56b-409a-988b-633d05296083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training points\n",
    "plot_colors = \"br\"\n",
    "plot_step = 0.02\n",
    "class_names = \"AB\"\n",
    "for i, n, c in zip(range(2), class_names, plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(\n",
    "        X[idx, 0],\n",
    "        X[idx, 1],\n",
    "        c=c,\n",
    "        cmap=plt.cm.Paired,\n",
    "        s=20,\n",
    "        edgecolor=\"k\",\n",
    "        label=\"Class %s\" % n,\n",
    "    )\n",
    "    plt.xlabel(r'$X_0$')\n",
    "    plt.ylabel(r'$X_1$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8f414-e40a-4e20-8c95-3d9e07140c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit an AdaBoosted decision tree - not creating splits for synthetic dataset (if you want to report how well your model does, you always should!)\n",
    "bdt = AdaBoostClassifier(\n",
    "    tree.DecisionTreeClassifier(max_depth=1), algorithm=\"SAMME\", n_estimators=2000, learning_rate=1e-2\n",
    ")\n",
    "bdt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf74beb-d61b-4752-a76f-7931ee7d40aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the plots\n",
    "plt.subplot(111)\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(\n",
    "    np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)\n",
    ")\n",
    "\n",
    "# Make predictions using fitted tree\n",
    "Z = bdt.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "plt.axis(\"tight\")\n",
    "\n",
    "# Plot the training points\n",
    "for i, n, c in zip(range(2), class_names, plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(\n",
    "        X[idx, 0],\n",
    "        X[idx, 1],\n",
    "        c=c,\n",
    "        cmap=plt.cm.Paired,\n",
    "        s=20,\n",
    "        edgecolor=\"k\",\n",
    "        label=\"Class %s\" % n,\n",
    "    )\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Decision Boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bbf4fb-c483-4c63-bbcb-173e35c425bf",
   "metadata": {},
   "source": [
    "## Part 2 : Perceptrons & Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d5c9da-a171-400a-8aee-8ba8d0448856",
   "metadata": {},
   "source": [
    "### Perceptrons using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52f3ace-ee28-4a11-8973-d389d69f29c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ed21b-78c9-4dc6-bcad-78c38ce30920",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Perceptron(tol=1e-3, random_state=0, shuffle=True)\n",
    "### This is equivalent to SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\", penalty=None)\n",
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)\n",
    "X_train_norm = (X_train - mean) / std\n",
    "clf.fit(X_train_norm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8776bda6-0f93-481e-b094-feac7ebeb884",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score((X_test- mean) / std, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02219b4-8b40-44b5-864d-837a118bd971",
   "metadata": {},
   "source": [
    "\n",
    "## What is PyTorch?\n",
    "\n",
    "Itâ€™s a Python based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "-  Tensorial library that uses the power of GPUs\n",
    "-  A deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "In this class though we will not use any GPU based computation since most of the work (labs and assignments) will be done on your laptops. \n",
    "\n",
    "## Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d416c934-5790-4c50-a4d5-7b5ba03fa028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # <Ctrl> / <Shift> + <Return>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341074c2-67d3-4a0f-91ba-70809413ec19",
   "metadata": {},
   "source": [
    "## The Torch Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83da52c7-b069-48a7-9770-aec64fa0a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a tensor of size 2x3x4\n",
    "t = torch.Tensor(2, 3, 4)\n",
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f84495-d4e8-4ca9-8067-77c89a1869cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the tensor\n",
    "print(t.size(), t.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce124014-2c54-4aa8-bdfa-4aae60251181",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Vectors (1D Tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c00a54-ace5-452d-905d-5e7f6ea722be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a 1D tensor of integers 1 to 4\n",
    "v = torch.Tensor([1, 2, 3, 4])\n",
    "\n",
    "# Create a 2x4 tensor\n",
    "m = torch.Tensor([[2, 5, 3, 7],\n",
    "                  [4, 2, 1, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f56224-d66e-4bad-9556-231a1d174335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of dimensions (1D) and size of tensor\n",
    "print(f'dim: {v.dim()}, size: {v.size()}')\n",
    "\n",
    "# Print number of dimensions (2D) and size of tensor\n",
    "print(f'dim: {m.dim()}, size: {m.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add36a31-88f6-44fe-98dd-8aa98ac6a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise multiplication\n",
    "v * m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae699d-0568-4bd9-be3c-2d6fd6690019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar product: 1*1 + 2*0 + 3*2 + 4*0\n",
    "m @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef4892-37d7-4326-94f0-9cea66836fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-place replacement of random number from 0 to 10\n",
    "x = torch.Tensor(5).random_(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85596b97-dad8-4d45-898c-b43bdd35c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'first: {x[0]}, last: {x[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f736ec-cd39-48d3-b06c-b7488f889306",
   "metadata": {},
   "source": [
    "### Constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b197428-df1c-49f9-98ac-aa59ac664401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor from 3 to 8, with each having a space of 1\n",
    "torch.arange(3., 8 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09500076-ad36-44eb-bf56-dbb390635bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor from 5.7 to -2.1 with each having a space of -3\n",
    "torch.arange(5.7, -2.1, -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b374e6df-0be8-4888-8535-0449d8c294db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a 1D tensor of steps equally spaced points between start=3, end=8 and steps=20\n",
    "torch.linspace(3, 8, 20).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72acd5d-94be-4161-81a7-0b64157b3967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor filled with 0's\n",
    "torch.zeros(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6359251d-1e24-44da-8347-8f234b15ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor filled with 1's\n",
    "torch.ones(3, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dcbdd1-0433-4f78-9154-0d903e537d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor with the diagonal filled with 1\n",
    "torch.eye(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f8030-dc40-4ae1-8beb-3fff165cfd56",
   "metadata": {},
   "source": [
    "### Defining a model in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a7720-78a8-48c1-8ea2-a0a49c9e28e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import math\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0112397-70c4-42e1-b252-91f82ba92929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture hyperparameters\n",
    "D = 4  # input dimensions\n",
    "C = 3  # num_classes\n",
    "H = 10  # num_hidden_units\n",
    "\n",
    "# Define training hyperparameters\n",
    "learning_rate = 1e-2\n",
    "lambda_l2 = 1e-1 # coefficient for the L2 regularizer. You should play with its value to see the effect of regularization\n",
    "\n",
    "# nn package to create our linear model. Notice the Sequential container class. \n",
    "# Each Linear module has a weight and bias\n",
    "# The order in which the Linear modules are defined is important as it creates the directed acyclic graph\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.Linear(H, C)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c847a36-99ec-4d3d-949d-54505c92d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88875751-d376-4852-a2c0-55cdf5ecdb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to preprocess our data and form tensors before we can use a PyTorch Model\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "\n",
    "# First move features present in pandas dataframes to tensors, this is straightforward\n",
    "X_train_pth = torch.as_tensor(X_train.values, dtype=torch.float)\n",
    "X_test_pth = torch.as_tensor(X_test.values, dtype=torch.float)\n",
    "\n",
    "feature_means = torch.mean(X_train_pth, dim = 0)\n",
    "feature_stds = torch.std(X_train_pth, dim=0)\n",
    "print(feature_means.shape, feature_stds.shape)\n",
    "\n",
    "# scaler = preprocessing.StandardScaler()\n",
    "# X_train_pth = torch.tensor(scaler.fit_transform(X_train_pth), dtype=torch.float32)\n",
    "\n",
    "X_train_pth = (X_train_pth - feature_means)/feature_stds\n",
    "\n",
    "# Convert the labels need a little more care since Pytorch loss functions do not expect string labels\n",
    "labels = y_train.unique() # Get all unique labels from our training set\n",
    "le = preprocessing.LabelEncoder() # Define encoder using sklearn's LabelEncoder\n",
    "targets = le.fit_transform(y_train) # Transform string targets to integers\n",
    "# targets: array([0, 1, 2, 3, 4])\n",
    "\n",
    "y_train_pth = torch.as_tensor(targets).long() # Finally put everything into a tensor\n",
    "\n",
    "# Repeat for test labels\n",
    "targets = le.fit_transform(y_test) # Transform string targets to integers\n",
    "\n",
    "y_test_pth = torch.as_tensor(targets).long()\n",
    "\n",
    "print(X_train_pth.shape,y_train_pth.shape, X_test_pth.shape, y_test_pth)\n",
    "print(torch.mean(X_train_pth,dim=0), torch.std(X_train_pth,dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e75fb8-ecec-411f-a3e6-5f0193dd6afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn package has a variety of loss functions already implemented\n",
    "# we use cross entropy loss for our classification task\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# nn package also has a variety of optimization algorithms implemented\n",
    "# we use the stochastic gradient descent for our parameter updates\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "# Training loop\n",
    "for t in range(1000):\n",
    "    \n",
    "    # Forward pass over the model to get the logits \n",
    "    y_pred = model(X_train_pth) #1\n",
    "    # print(y_pred)\n",
    "    # Compute the loss and accuracy\n",
    "    loss = criterion(y_pred, y_train_pth) #2\n",
    "    \n",
    "    # print(y_pred, y_train_pth)\n",
    "    # break\n",
    "    score, predicted = torch.max(y_pred, 1)\n",
    "    acc = (y_train_pth == predicted).sum() / len(y_train)\n",
    "    print(\"[EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, loss.item(), acc))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # reset (zero) the gradients before running the backward pass over the model\n",
    "    # we need to do this because the gradients get accumulated at the same place across iterations\n",
    "    optimizer.zero_grad() #3\n",
    "    \n",
    "    # Backward pass to compute the gradient of loss w.r.t our learnable params (weights and biases)\n",
    "    loss.backward() #4\n",
    "    \n",
    "    # Update params\n",
    "    optimizer.step() #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea627c58-9579-4a86-a9a8-f5dd0bcc9044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "out = model(X_test_pth)\n",
    "_, predicted = torch.max(out.data, 1)\n",
    "\n",
    "#get accuration\n",
    "print('Accuracy of the network %.4f %%' % (100 * torch.sum(y_test_pth==predicted).double() / len(y_test_pth)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8493be30-22d6-495f-9674-676722f4ce9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Classification of non-linearly separable dataset using multi-layer perceptron\n",
    "We will not demonstrate how to build a linear model and a neural network model using PyTorch and how to train it to classify a toy dataset which is not linearly separable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53dfd41-55ab-4bea-ac85-718da55daa80",
   "metadata": {},
   "source": [
    "Import appropriate packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2130a8ac-05d2-424a-9602-862e11fdafd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some predefined helper functions provided for plotting data and model outputs\n",
    "from plot_lib import plot_data, plot_model, set_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5a025-7683-4c48-a493-3c2c32a4fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f8e2a-519f-4674-8d15-fb4dbdb8a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12345\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "N = 1000  # num_samples_per_class\n",
    "D = 2  # dimensions\n",
    "C = 3  # num_classes\n",
    "H = 100  # num_hidden_units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc061da-66c9-4ad0-a4be-e745a753a122",
   "metadata": {},
   "source": [
    "### Create the dataset\n",
    "We will now create a data set consisting of three classes and is in the shape of a spiral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55fb1ed-bb2b-48fe-b191-93140c7bb31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "X = torch.zeros(N * C, D).to(device)\n",
    "y = torch.zeros(N * C, dtype=torch.long).to(device)\n",
    "for c in range(C):\n",
    "    index = 0\n",
    "    t = torch.linspace(0, 1, N)\n",
    "    # When c = 0 and t = 0: start of linspace\n",
    "    # When c = 0 and t = 1: end of linpace\n",
    "    # This inner_var is for the formula inside sin() and cos() like sin(inner_var) and cos(inner_Var)\n",
    "    inner_var = torch.linspace(\n",
    "        # When t = 0\n",
    "        (2 * math.pi / C) * (c),\n",
    "        # When t = 1\n",
    "        (2 * math.pi / C) * (2 + c),\n",
    "        N\n",
    "    ) + torch.randn(N) * 0.2\n",
    "    \n",
    "    for ix in range(N * c, N * (c + 1)):\n",
    "        X[ix] = t[index] * torch.FloatTensor((\n",
    "            math.sin(inner_var[index]), math.cos(inner_var[index])\n",
    "        ))\n",
    "        y[ix] = c\n",
    "        index += 1\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"X:\", tuple(X.size()))\n",
    "print(\"y:\", tuple(y.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac8a24-6350-4288-9e5b-51f963b9b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the data using the plot_data function provided as a helper function\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3df934-ff79-4779-9af0-b5fedfa51885",
   "metadata": {},
   "source": [
    "### Linear Model\n",
    "We now define a linear classification model using PyTorch and train it using stochastic gradient descent with the help of the autograd package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abac377-ac72-4876-a095-25bcd8a9a5e0",
   "metadata": {},
   "source": [
    "Initialize some hyper-parameter values, such as, learning rate, regularization coefficient etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa718848-14b3-4c80-bad9-4b9310c8e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "lambda_l2 = 1e-3 # coefficient for the L2 regularizer. You should play with its value to see the effect of regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af4d3b-caab-4fb1-9f73-7ae23bb4c28f",
   "metadata": {},
   "source": [
    "Create the linear model and print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed2754-2b40-493a-b2a8-bb1fdbc4bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn package to create our linear model. Notice the Sequential container class. \n",
    "# Each Linear module has a weight and bias\n",
    "# The order in which the Linear modules are defined is important as it creates the directed acyclic graph\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.Linear(H, C)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9838b39c-1ba3-417d-83a9-9f3b0ab24dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349e5e3f-628a-4965-b68f-0d33b19af194",
   "metadata": {},
   "source": [
    "Create the training loop and print the metrics (loss and accuracy) at the end of each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5ac326-8603-4231-9453-a89652deffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn package has a variety of loss functions already implemented\n",
    "# we use cross entropy loss for our classification task\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# nn package also has a variety of optimization algorithms implemented\n",
    "# we use the stochastic gradient descent for our parameter updates\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "# Training loop\n",
    "for t in range(1000):\n",
    "    \n",
    "    # Forward pass over the model to get the logits \n",
    "    y_pred = model(X) #1\n",
    "    \n",
    "    # Compute the loss and accuracy\n",
    "    loss = criterion(y_pred, y) #2\n",
    "    # score, predicted = torch.max(y_pred, 1)\n",
    "    # acc = (y == predicted).sum().float() / len(y)\n",
    "    print(\"[EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, loss.item(), acc))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # reset (zero) the gradients before running the backward pass over the model\n",
    "    # we need to do this because the gradients get accumulated at the same place across iterations\n",
    "    optimizer.zero_grad() #3\n",
    "    \n",
    "    # Backward pass to compute the gradient of loss w.r.t our learnable params (weights and biases)\n",
    "    loss.backward() #4\n",
    "    \n",
    "    # Update params\n",
    "    optimizer.step() #5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2708fb76-f27f-417c-93d0-6bd66c2ef9f3",
   "metadata": {},
   "source": [
    "Plot the output of the model (in this case a collection of hyper-planes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbe03c-99a2-4508-a32c-29ce2c964c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trained model\n",
    "plot_model(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae1e713-88ba-47d5-b79e-0b753919cc08",
   "metadata": {},
   "source": [
    "### Two layer neural network\n",
    "We now define a two layer (single hidden layer) neural network model using PyTorch and train it using stochastic gradient descent with the help of the autograd package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a90844e-1d98-4ebd-92b0-0083655b0ecb",
   "metadata": {},
   "source": [
    "Initialize the hyper-parameters like before. Play around with the learning rate and the regularization parameter to see their effect on the optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b9cfdb-4ecc-47c4-b73f-b9b04460d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "lambda_l2 = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d597e63-0f69-43e6-b1cc-0e77eed1942b",
   "metadata": {},
   "source": [
    "Create and print the two layer MLP with ReLU as the activation units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d11d1-48d4-4e1b-a7d9-310b2a43f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn package to create our linear model\n",
    "# each Linear module has a weight and bias\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H,C)\n",
    "    \n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b8a581-6dc6-424a-980a-e840a67661b4",
   "metadata": {},
   "source": [
    "Create the training loop and print the metrics (loss and accuracy) at the end of each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219310df-1969-48fe-b419-a2679550a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn package has a variety of loss functions already implemented\n",
    "# we use cross entropy loss for our classification task\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# nn package also has a variety of optimization algorithms implemented\n",
    "# we use the stochastic gradient descent for our parameter updates\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "# e = 1.  # plotting purpose\n",
    "\n",
    "# Training\n",
    "for t in range(1000):\n",
    "    \n",
    "    # Forward pass over the model to get the logits\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # Compute the loss and accuracy\n",
    "    loss = criterion(y_pred, y)\n",
    "    score, predicted = torch.max(y_pred, 1)\n",
    "    acc = (y == predicted).sum().float() / len(y)\n",
    "    print(\"[EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, loss.item(), acc))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # reset (zero) the gradients before running the backward pass over the model\n",
    "    # we need to do this because the gradients get accumulated at the same place across iterations\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass to compute the gradient of loss w.r.t our learnable params (weights and biases)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update params\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22202340-64db-4303-89fe-ccbda743a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trained model\n",
    "plot_model(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a570291-f5f5-4c90-aef5-d266efa3df57",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "We will now demonstrate how to build a linear model and a neural network model for a regression task using PyTorch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b214afc2-c920-449a-b73a-8cc16b9bcb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import math\n",
    "from IPython import display\n",
    "from plot_lib import plot_data, plot_model, set_default\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19321a5-a958-49d4-aaf8-55cd8a9b6848",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca6fb3b-11a2-482a-8a5b-cdec77d75eed",
   "metadata": {},
   "source": [
    "### Create the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763602d0-ff7c-4977-992b-9136c590c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "N = 1000  # num_samples_per_class\n",
    "D = 1  # dimensions\n",
    "C = 1  # num_classes\n",
    "H = 100  # num_hidden_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa8e8b-4fb3-464d-89c2-68970f9db4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1).to(device)\n",
    "y = X.pow(3) + 0.3 * torch.rand(X.size()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7ea2d6-6ea0-493f-83ed-e380746a91de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shapes:\")\n",
    "print(\"X:\", tuple(X.size()))\n",
    "print(\"y:\", tuple(y.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb58d1b-5373-4ab8-89dc-252ef7a08e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.cpu().numpy(), y.cpu().numpy())\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397dbf76-40b0-45ae-8c74-19f99a80446c",
   "metadata": {},
   "source": [
    "### Linear model\n",
    "\n",
    "Initialize the values of the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10547b32-acf8-4ea1-bf37-638f5a6986da",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "lambda_l2 = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49072c8-cdab-4a03-ba52-14b932482bf3",
   "metadata": {},
   "source": [
    "Create the model and print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb6a4a0-93d3-4bec-a761-0f004adc379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn package to create our linear model\n",
    "# each Linear module has a weight and bias\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.Linear(H, C)\n",
    ")\n",
    "\n",
    "# print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47259e7b-4833-48cf-bc67-cfb38da227e9",
   "metadata": {},
   "source": [
    "Create the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4bbd6c-9019-4f65-a9de-43f72ada1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use MSE (mean squared error) loss from the nn package for our regression task\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# we use the optim package to apply stochastic gradient descent for our parameter updates\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "# Training loop\n",
    "for t in range(1000):\n",
    "    \n",
    "    # forward pass over the model to get the logits (inputs to the loss function)\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # Compute the loss (MSE)\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(\"[EPOCH]: %i, [LOSS or MSE]: %.6f\" % (t, loss.item()))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # zero the gradients before running the backward pass\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass to compute the gradient of loss w.r.t our learnable params \n",
    "    loss.backward()\n",
    "    \n",
    "    # Update params\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff37e8af-e42a-48dd-bb8f-3ef44b38cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trained model\n",
    "plt.scatter(X.data.cpu().numpy(), y.data.cpu().numpy())\n",
    "plt.plot(X.data.cpu().numpy(), y_pred.data.cpu().numpy(), 'r-', lw=5)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5f063f-35b4-4ea9-bff2-728a2cd6cb08",
   "metadata": {},
   "source": [
    "Play around with the values of the hyper-parameters and the value of H (number of hidden units) to observe the change in the models being learnt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
